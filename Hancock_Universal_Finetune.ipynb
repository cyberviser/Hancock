{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hancock_Universal_Finetune.ipynb",
      "gpuType": "T4",
      "accelerator": "GPU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cyberviser/Hancock/blob/main/Hancock_Universal_Finetune.ipynb)\n\n# \ud83d\udd10 Hancock Universal Fine-Tuning \u2014 CyberViser\n**Mistral 7B \u2192 Cybersecurity specialist via LoRA**\n\nWorks on: **Google Colab** (free T4) \u00b7 **Kaggle** (free T4, 30h/week) \u00b7 **RunPod/Vast** \u00b7 **Oracle Cloud**\n\n| Step | Time | Notes |\n|------|------|-------|\n| Install deps | ~3 min | Unsloth + TRL |\n| Load 4-bit model | ~2 min | 7B params, 4GB VRAM |\n| Train (300 steps) | ~25 min | v3 dataset (5,670 samples) |\n| Export GGUF Q4 | ~5 min | Ready for Ollama |\n\n**Setup:**\n- **Colab:** Runtime \u2192 Change runtime type \u2192 T4 GPU\n- **Kaggle:** Settings \u2192 Accelerator \u2192 GPU T4 x2 \u00b7 Internet \u2192 On\n- **Optional:** Add `HF_TOKEN` secret to push model to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 1\ufe0f\u20e3  Detect Environment\nimport os, platform\n\nENV = 'Unknown'\nif os.path.exists('/kaggle'):\n    ENV = 'Kaggle'\n    WORK_DIR = '/kaggle/working'\nelif 'COLAB_GPU' in os.environ or os.path.exists('/content'):\n    ENV = 'Colab'\n    WORK_DIR = '/content'\nelse:\n    ENV = 'Cloud/Local'\n    WORK_DIR = os.getcwd()\n\nprint(f'\ud83d\udcbb Environment: {ENV}')\nprint(f'\ud83d\udcc1 Work dir: {WORK_DIR}')\nos.chdir(WORK_DIR)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 2\ufe0f\u20e3  Install Dependencies (~3 min)\nimport subprocess, sys\n\n# Unsloth install varies by environment\nif ENV == 'Kaggle':\n    !pip install 'unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git' -q\nelse:\n    !pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git' -q\n\n!pip install -q 'trl>=0.8.6' 'transformers>=4.40' 'datasets>=2.18' peft huggingface_hub accelerate bitsandbytes\n\nimport importlib, pkg_resources\nfor pkg in ['unsloth','trl','transformers','datasets','peft']:\n    v = pkg_resources.get_distribution(pkg).version\n    print(f'  {pkg}: {v}')\nprint('\\n\u2705 Dependencies installed')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 3\ufe0f\u20e3  Clone Hancock & Check GPU\nimport torch\n\n# Clone repo\nif not os.path.exists(os.path.join(WORK_DIR, 'Hancock')):\n    !git clone https://github.com/cyberviser/Hancock.git {WORK_DIR}/Hancock\nos.chdir(os.path.join(WORK_DIR, 'Hancock'))\n\n# GPU check\nassert torch.cuda.is_available(), '\u274c Enable GPU runtime first!'\ngpu = torch.cuda.get_device_name(0)\nvram = torch.cuda.get_device_properties(0).total_memory / 1e9\nprint(f'GPU: {gpu} | VRAM: {vram:.1f} GB')\nprint(f'Repo: {os.getcwd()}')\nprint('\u2705 Ready')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 4\ufe0f\u20e3  Configure Training\n# Adjust these settings before running:\n\nTRAINING_STEPS = 300        # More steps = better quality, longer time (~5 min per 100 steps on T4)\nEXPORT_GGUF = True          # Export GGUF for Ollama deployment\nPUSH_TO_HUB = False         # Push to HuggingFace Hub (requires HF_TOKEN)\nLORA_RANK = 0               # 0 = auto-detect based on VRAM (16=T4, 32=24GB, 64=40GB+)\n\nprint(f'Steps: {TRAINING_STEPS}')\nprint(f'GGUF export: {EXPORT_GGUF}')\nprint(f'Push to Hub: {PUSH_TO_HUB}')\nprint(f'LoRA rank: {\"auto\" if LORA_RANK == 0 else LORA_RANK}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 5\ufe0f\u20e3  Run Fine-Tuning (~25 min on T4)\n# Uses the universal hancock_finetune_v3.py script\n\ncmd = f'python hancock_finetune_v3.py --steps {TRAINING_STEPS}'\nif EXPORT_GGUF:\n    cmd += ' --export-gguf'\nif PUSH_TO_HUB:\n    cmd += ' --push-to-hub'\nif LORA_RANK > 0:\n    cmd += f' --lora-r {LORA_RANK}'\n\nprint(f'Running: {cmd}\\n')\n!{cmd}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 6\ufe0f\u20e3  Test the Fine-Tuned Model\nimport torch\nfrom unsloth import FastLanguageModel\nfrom pathlib import Path\n\nadapter_dir = 'hancock-adapter-v3'\nassert Path(adapter_dir).exists(), f'Adapter not found at {adapter_dir}'\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=adapter_dir,\n    max_seq_length=4096,\n    dtype=None,\n    load_in_4bit=True,\n)\nFastLanguageModel.for_inference(model)\n\nmessages = [\n    {'role': 'system', 'content': 'You are Hancock, an elite cybersecurity AI by CyberViser.'},\n    {'role': 'user', 'content': 'Explain CVE-2021-44228 Log4Shell and provide a Splunk SPL query to detect exploitation attempts.'},\n]\ninputs = tokenizer.apply_chat_template(\n    messages, tokenize=True, add_generation_prompt=True, return_tensors='pt'\n).to('cuda')\n\nwith torch.no_grad():\n    outputs = model.generate(\n        input_ids=inputs, max_new_tokens=512,\n        use_cache=True, temperature=0.7, do_sample=True,\n    )\n\nresponse = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\nprint('\\n\ud83d\udee1\ufe0f Hancock says:')\nprint('=' * 60)\nprint(response)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 7\ufe0f\u20e3  Download Model Files\nimport os\nfrom pathlib import Path\n\nprint('\\n\ud83d\udce6 Available model files:\\n')\n\n# LoRA adapter\nadapter_dir = Path('hancock-adapter-v3')\nif adapter_dir.exists():\n    size = sum(f.stat().st_size for f in adapter_dir.rglob('*') if f.is_file()) / 1e6\n    print(f'  \ud83d\udcc1 hancock-adapter-v3/ ({size:.0f} MB) \u2014 LoRA adapter')\n\n# GGUF\nfor gguf in Path('.').glob('*.gguf'):\n    size = gguf.stat().st_size / 1e6\n    print(f'  \ud83d\udcc1 {gguf.name} ({size:.0f} MB) \u2014 GGUF for Ollama')\n\nprint('\\n\u2b07\ufe0f  Download options:')\nif ENV == 'Colab':\n    print('  Option 1: Run the cell below to download via browser')\n    print('  Option 2: Mount Google Drive and copy there')\nelif ENV == 'Kaggle':\n    print('  Option 1: Output tab \u2192 download files from /kaggle/working/Hancock/')\n    print('  Option 2: Push to HuggingFace Hub (re-run with PUSH_TO_HUB=True)')\nelse:\n    print('  Files are saved in the current directory')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 8\ufe0f\u20e3  Download GGUF (Colab only)\n# Skip this cell on Kaggle \u2014 use Output tab instead\nimport os\n\ntry:\n    from google.colab import files\n    for f in os.listdir('.'):\n        if f.endswith('.gguf'):\n            print(f'Downloading {f}...')\n            files.download(f)\n            break\n    else:\n        print('No GGUF file found. Run with EXPORT_GGUF=True')\nexcept ImportError:\n    print('Not running on Colab \u2014 download manually from the Output tab (Kaggle) or local filesystem')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## \ud83d\ude80 Deploy with Ollama (after download)\n\n```bash\n# On your local machine:\nmkdir -p ~/cyberviser/models && mv hancock-v3-Q4_K_M.gguf ~/cyberviser/models/\ncd ~/cyberviser/Hancock\n\n# Update Modelfile to point to GGUF:\n# FROM ./models/hancock-v3-Q4_K_M.gguf\n\nollama create hancock-finetuned -f Modelfile.hancock-finetuned\nollama run hancock-finetuned\n```\n\n---\n\u00a9 2026 CyberViser \u00b7 [cyberviser.netlify.app](https://cyberviser.netlify.app)"
      ]
    }
  ]
}
