{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hancock_Finetune_v3.ipynb",
      "gpuType": "T4",
      "accelerator": "GPU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cyberviser/Hancock/blob/main/Hancock_Colab_Finetune_v3.ipynb)\n\n# \ud83d\udd10 Hancock Fine-Tuning \u2014 CyberViser\n**Mistral 7B \u2192 Cybersecurity specialist via LoRA**\n\n| Step | Time | Notes |\n|------|------|-------|\n| Install deps | ~3 min | Unsloth + TRL |\n| Load 4-bit model | ~2 min | 7B params, 4GB VRAM |\n| Train 3 epochs | ~45 min | v3 dataset on T4 (CISA KEV + Atomic + GHSA) |\n| Export GGUF Q4 | ~5 min | Ready for Ollama |\n\n> **Runtime \u2192 Change runtime type \u2192 T4 GPU** before running!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 1\ufe0f\u20e3  Install Dependencies\n# Pin versions for compatibility\n!pip install -q \"unsloth[colab-new]\" \\\n    \"trl>=0.8.6,<0.10\" \\\n    \"transformers>=4.40,<4.46\" \\\n    \"datasets>=2.18\" \\\n    \"peft\" \"huggingface_hub\" \"accelerate\"\nimport importlib, pkg_resources\nfor pkg in ['unsloth','trl','transformers','datasets','peft']:\n    v = pkg_resources.get_distribution(pkg).version\n    print(f'  {pkg}: {v}')\nprint('\u2705 Deps installed')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 2\ufe0f\u20e3  Clone Hancock Repo\nimport os\n!git clone https://github.com/cyberviser/Hancock.git /content/Hancock\nos.chdir('/content/Hancock')\nprint('\u2705 Repo cloned')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 3\ufe0f\u20e3  Check GPU\nimport torch\ngpu = torch.cuda.get_device_name(0)\nvram = torch.cuda.get_device_properties(0).total_memory / 1e9\nprint(f'GPU: {gpu} | VRAM: {vram:.1f} GB')\nassert torch.cuda.is_available(), 'Enable GPU runtime first!'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 4\ufe0f\u20e3  Load Training Data\nimport json\nfrom pathlib import Path\n\n# Try v3 first (CISA KEV + Atomic + GHSA), fall back to v2\ndataset_path = Path('data/hancock_v3.jsonl')\nif not dataset_path.exists():\n    dataset_path = Path('data/hancock_v2.jsonl')\nif not dataset_path.exists():\n    print('Generating v3 dataset (takes ~5 min)...')\n    !python hancock_pipeline.py --phase 3\n    dataset_path = Path('data/hancock_v3.jsonl')\n    if not dataset_path.exists():\n        dataset_path = Path('data/hancock_v2.jsonl')\n\nlines = dataset_path.read_text().strip().splitlines()\ndata  = [json.loads(l) for l in lines]\nprint(f'\u2705 Loaded {len(data):,} training samples from {dataset_path.name}')\nprint('Sample:', json.dumps(data[0]['messages'][1], indent=2)[:200])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 5\ufe0f\u20e3  Load Mistral 7B (4-bit)\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name     = 'mistralai/Mistral-7B-Instruct-v0.3',\n    max_seq_length = 2048,\n    dtype          = None,\n    load_in_4bit   = True,\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n    lora_alpha=32, lora_dropout=0.05, bias='none',\n    use_gradient_checkpointing='unsloth', random_state=42,\n)\nprint(f'\u2705 Trainable params: {model.num_parameters(only_trainable=True):,}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 6\ufe0f\u20e3  Format Dataset\nfrom datasets import Dataset\n\ntexts = [\n    tokenizer.apply_chat_template(s['messages'], tokenize=False, add_generation_prompt=False)\n    for s in data\n]\nds = Dataset.from_dict({'text': texts}).train_test_split(test_size=0.05, seed=42)\nprint(f'Train: {len(ds[\"train\"]):,} | Eval: {len(ds[\"test\"]):,}')\nprint('\\nSample formatted text:')\nprint(texts[0][:400])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 7\ufe0f\u20e3  Train (~45 min on T4)\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size  = 2,\n    gradient_accumulation_steps  = 4,\n    warmup_ratio                 = 0.05,\n    num_train_epochs             = 3,\n    learning_rate                = 2e-4,\n    fp16                         = not torch.cuda.is_bf16_supported(),\n    bf16                         = torch.cuda.is_bf16_supported(),\n    logging_steps                = 20,\n    eval_strategy                = 'steps',  # transformers>=4.45\n    eval_steps                   = 100,\n    save_strategy                = 'steps',\n    save_steps                   = 200,\n    save_total_limit             = 2,\n    output_dir                   = '/content/hancock_checkpoints',\n    report_to                    = 'none',\n    optim                        = 'adamw_8bit',\n    weight_decay                 = 0.01,\n    lr_scheduler_type            = 'cosine',\n    seed                         = 42,\n)\n\ntrainer = SFTTrainer(\n    model               = model,\n    tokenizer           = tokenizer,\n    train_dataset       = ds['train'],\n    eval_dataset        = ds['test'],\n    dataset_text_field  = 'text',\n    max_seq_length      = 2048,\n    packing             = True,\n    args                = training_args,\n)\n\n# Show initial memory usage\ngpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\nused    = torch.cuda.memory_allocated(0) / 1e9\nprint(f'VRAM: {used:.1f}/{gpu_mem:.1f} GB used before training')\n\nresult = trainer.train()\nprint(f'\\n\u2705 Done! Final loss: {result.training_loss:.4f}')\nprint(f'Steps: {result.global_step} | Samples/sec: {result.training_loss:.4f}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 8\ufe0f\u20e3  Save LoRA + GGUF Q4\nimport os\n\n# Always save LoRA adapters (fast, always works)\nmodel.save_pretrained('/content/hancock_lora')\ntokenizer.save_pretrained('/content/hancock_lora')\nprint('\u2705 LoRA adapters saved \u2192 /content/hancock_lora')\nprint(f'   Size: {sum(os.path.getsize(os.path.join(\"/content/hancock_lora\",f)) for f in os.listdir(\"/content/hancock_lora\")) / 1e6:.1f} MB')\n\n# Export GGUF Q4_K_M (requires ~10 min + llama.cpp build)\ntry:\n    model.save_pretrained_gguf('/content/hancock_gguf', tokenizer,\n                               quantization_method='q4_k_m')\n    print('\u2705 GGUF Q4_K_M saved \u2192 /content/hancock_gguf')\nexcept Exception as e:\n    print(f'\u26a0\ufe0f  GGUF export failed (non-fatal): {e}')\n    print('   LoRA adapters are saved \u2014 use them directly or convert later.')\n    print('   Convert offline: python -m llama_cpp.convert ...')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 9\ufe0f\u20e3  Test the Fine-Tuned Model\nfrom unsloth import FastLanguageModel\nFastLanguageModel.for_inference(model)\n\nmessages = [\n    {'role': 'system', 'content': 'You are Hancock, an elite cybersecurity AI by CyberViser.'},\n    {'role': 'user', 'content': 'Explain CVE-2021-44228 Log4Shell and how to detect it in Splunk.'},\n]\ninputs = tokenizer.apply_chat_template(\n    messages, tokenize=True, add_generation_prompt=True, return_tensors='pt'\n).to('cuda')\n\nwith torch.no_grad():\n    outputs = model.generate(\n        input_ids=inputs, max_new_tokens=512,\n        use_cache=True, temperature=0.7, do_sample=True,\n    )\n\n# Decode only the generated tokens (not the prompt)\nresponse = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\nprint('Hancock says:')\nprint('=' * 60)\nprint(response)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title \ud83d\udd1f  Push to HuggingFace Hub (optional)\nHF_TOKEN = ''  # @param {type:'string'}\nif HF_TOKEN:\n    model.push_to_hub('cyberviser/hancock-mistral-7b', token=HF_TOKEN)\n    tokenizer.push_to_hub('cyberviser/hancock-mistral-7b', token=HF_TOKEN)\n    print('\u2705 Pushed to https://huggingface.co/cyberviser/hancock-mistral-7b')\nelse:\n    print('Skipped \u2014 add your HF_TOKEN to push')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 1\ufe0f\u20e31\ufe0f\u20e3  Download GGUF to local (for Ollama)\nfrom google.colab import files\nimport os\nfor f in os.listdir('/content/hancock_gguf'):\n    if f.endswith('.gguf'):\n        print(f'Downloading {f}...')\n        files.download(f'/content/hancock_gguf/{f}')"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}