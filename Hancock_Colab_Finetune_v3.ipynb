{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hancock_Finetune_v3.ipynb",
      "gpuType": "T4",
      "accelerator": "GPU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd10 Hancock Fine-Tuning \u2014 CyberViser\n**Mistral 7B \u2192 Cybersecurity specialist via LoRA**\n\n| Step | Time | Notes |\n|------|------|-------|\n| Install deps | ~3 min | Unsloth + TRL |\n| Load 4-bit model | ~2 min | 7B params, 4GB VRAM |\n| Train 3 epochs | ~45 min | 1,375 samples on T4 |\n| Export GGUF Q4 | ~5 min | Ready for Ollama |\n\n> **Runtime \u2192 Change runtime type \u2192 T4 GPU** before running!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 1\ufe0f\u20e3  Install Dependencies\n!pip install -q \"unsloth[colab-new]\" trl transformers datasets peft huggingface_hub\nprint('\u2705 Deps installed')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 2\ufe0f\u20e3  Clone Hancock Repo\nimport os\n!git clone https://github.com/cyberviser/Hancock.git /content/Hancock\nos.chdir('/content/Hancock')\nprint('\u2705 Repo cloned')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 3\ufe0f\u20e3  Check GPU\nimport torch\ngpu = torch.cuda.get_device_name(0)\nvram = torch.cuda.get_device_properties(0).total_memory / 1e9\nprint(f'GPU: {gpu} | VRAM: {vram:.1f} GB')\nassert torch.cuda.is_available(), 'Enable GPU runtime first!'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 4\ufe0f\u20e3  Load Training Data\nimport json\nfrom pathlib import Path\n\ndataset_path = Path('data/hancock_v2.jsonl')\nif not dataset_path.exists():\n    print('Generating dataset...')\n    !python hancock_pipeline.py\n\nlines = dataset_path.read_text().strip().splitlines()\ndata  = [json.loads(l) for l in lines]\nprint(f'\u2705 Loaded {len(data):,} training samples')\nprint('Sample:', json.dumps(data[0]['messages'][1], indent=2)[:200])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 5\ufe0f\u20e3  Load Mistral 7B (4-bit)\nfrom unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name     = 'mistralai/Mistral-7B-Instruct-v0.3',\n    max_seq_length = 2048,\n    dtype          = None,\n    load_in_4bit   = True,\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n    lora_alpha=32, lora_dropout=0.05, bias='none',\n    use_gradient_checkpointing='unsloth', random_state=42,\n)\nprint(f'\u2705 Trainable params: {model.num_parameters(only_trainable=True):,}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 6\ufe0f\u20e3  Format Dataset\nfrom datasets import Dataset\n\ntexts = [\n    tokenizer.apply_chat_template(s['messages'], tokenize=False, add_generation_prompt=False)\n    for s in data\n]\nds = Dataset.from_dict({'text': texts}).train_test_split(test_size=0.05, seed=42)\nprint(f'Train: {len(ds[\"train\"]):,} | Eval: {len(ds[\"test\"]):,}')\nprint('\\nSample formatted text:')\nprint(texts[0][:400])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 7\ufe0f\u20e3  Train (~45 min on T4)\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model=model, tokenizer=tokenizer,\n    train_dataset=ds['train'], eval_dataset=ds['test'],\n    dataset_text_field='text', max_seq_length=2048, packing=True,\n    args=TrainingArguments(\n        per_device_train_batch_size=2, gradient_accumulation_steps=4,\n        warmup_ratio=0.05, num_train_epochs=3, learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=20, evaluation_strategy='steps', eval_steps=100,\n        save_strategy='steps', save_steps=200, save_total_limit=2,\n        output_dir='/content/hancock_checkpoints', report_to='none',\n        optim='adamw_8bit', weight_decay=0.01,\n        lr_scheduler_type='cosine', seed=42,\n    ),\n)\nresult = trainer.train()\nprint(f'\\n\u2705 Done! Final loss: {result.training_loss:.4f}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 8\ufe0f\u20e3  Save LoRA + GGUF Q4\nmodel.save_pretrained('/content/hancock_lora')\ntokenizer.save_pretrained('/content/hancock_lora')\nprint('\u2705 LoRA saved \u2192 /content/hancock_lora')\n\nmodel.save_pretrained_gguf('/content/hancock_gguf', tokenizer, quantization_method='q4_k_m')\nprint('\u2705 GGUF Q4_K_M saved \u2192 /content/hancock_gguf')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 9\ufe0f\u20e3  Test the Fine-Tuned Model\nfrom unsloth import FastLanguageModel\nFastLanguageModel.for_inference(model)\n\nmessages = [\n    {'role': 'system', 'content': 'You are Hancock, an elite cybersecurity AI by CyberViser.'},\n    {'role': 'user', 'content': 'Explain CVE-2021-44228 Log4Shell and how to detect it in Splunk.'},\n]\ninputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to('cuda')\noutputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True)\nprint(tokenizer.batch_decode(outputs)[0].split('[/INST]')[-1].strip())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title \ud83d\udd1f  Push to HuggingFace Hub (optional)\nHF_TOKEN = ''  # @param {type:'string'}\nif HF_TOKEN:\n    model.push_to_hub('cyberviser/hancock-mistral-7b', token=HF_TOKEN)\n    tokenizer.push_to_hub('cyberviser/hancock-mistral-7b', token=HF_TOKEN)\n    print('\u2705 Pushed to https://huggingface.co/cyberviser/hancock-mistral-7b')\nelse:\n    print('Skipped \u2014 add your HF_TOKEN to push')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title 1\ufe0f\u20e31\ufe0f\u20e3  Download GGUF to local (for Ollama)\nfrom google.colab import files\nimport os\nfor f in os.listdir('/content/hancock_gguf'):\n    if f.endswith('.gguf'):\n        print(f'Downloading {f}...')\n        files.download(f'/content/hancock_gguf/{f}')"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}