{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udee1\ufe0f Hancock v3 \u2014 CyberViser Fine-Tune\n",
        "**Mistral 7B \u00b7 LoRA (Unsloth) \u00b7 MITRE ATT&CK + NVD/CVE dataset**\n\n",
        "Runs free on: Google Colab (T4) \u00b7 Kaggle (P100) \u00b7 SageMaker Studio Lab (T4)\n\n",
        "> **Runtime:** Runtime \u2192 Change runtime type \u2192 **GPU (T4)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 Step 1: Check GPU \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 Step 2: Install dependencies \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "!pip install -q 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'\n",
        "!pip install -q trl>=0.8.0 transformers>=4.40.0 accelerate datasets bitsandbytes peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 Step 3: Clone Hancock \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "import os\n",
        "if not os.path.exists('Hancock'):\n",
        "    !git clone https://github.com/cyberviser/Hancock.git\n",
        "%cd Hancock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 Step 4: Set HuggingFace token (optional \u2014 for saving model) \u2500\u2500\u2500\n",
        "import os\n",
        "# Get free token at: https://huggingface.co/settings/tokens\n",
        "os.environ['HF_TOKEN'] = 'hf_your_token_here'  # \u2190 paste your HF token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 Step 5: Dry run \u2014 verify GPU + dataset load \u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "!python hancock_finetune_v3.py --dry-run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 Step 6: TRAIN \ud83d\ude80 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "# T4/P100 (free): 300 steps \u2248 25-35 min\n",
        "# Increase --steps for better quality (500 = ~1hr on T4)\n",
        "!python hancock_finetune_v3.py \\\n",
        "    --steps 300 \\\n",
        "    --push-to-hub \\\n",
        "    --export-gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 Step 7: Test the fine-tuned adapter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name='hancock-adapter-v3',\n",
        "    max_seq_length=4096,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "SYSTEM = 'You are Hancock, an elite cybersecurity specialist built by CyberViser.'\n",
        "messages = [\n",
        "    {'role': 'system',  'content': SYSTEM},\n",
        "    {'role': 'user',    'content': 'Explain CVE-2024-6387 and how to detect exploitation attempts.'},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=512, temperature=0.7)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2500\u2500 Step 8: Download GGUF for local deployment \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "# After training, download the GGUF file to run Hancock locally\n",
        "# with llama.cpp, Ollama, or LM Studio \u2014 no GPU needed!\n",
        "from google.colab import files\n",
        "import glob\n",
        "gguf = glob.glob('*.gguf')\n",
        "if gguf:\n",
        "    print(f'Downloading {gguf[0]}...')\n",
        "    files.download(gguf[0])\n",
        "else:\n",
        "    print('No GGUF found \u2014 re-run Step 6 with --export-gguf')"
      ]
    }
  ]
}