# CyberViser — Hancock Environment Configuration
# Copy this file to .env and fill in your values
# NEVER commit .env to git

# ── Backend Selection ─────────────────────────────────────────
# Options: groq | together | openrouter | ollama | nvidia | openai
#
# FREE cloud options (no GPU required, just an API key):
#   groq       — ~14,400 req/day FREE, 300-800 tok/s — https://console.groq.com
#   together   — $1-$25 free credits, Mistral/Llama/Qwen — https://api.together.xyz
#   openrouter — free rotating models (100+ models) — https://openrouter.ai
#
# Local option:
#   ollama     — install Ollama and run models locally — https://ollama.com
#
# Paid cloud options:
#   nvidia     — NVIDIA NIM (free credits, then paid) — https://build.nvidia.com
#   openai     — OpenAI API — https://platform.openai.com
HANCOCK_LLM_BACKEND=groq

# ── Groq (FREE — recommended for cloud deployment) ────────────
# Sign up free at: https://console.groq.com — no credit card needed
# Free tier: ~14,400 requests/day, 30 req/min
# Models: llama-3.3-70b-versatile | mixtral-8x7b-32768 | gemma2-9b-it
GROQ_API_KEY=gsk_your-groq-key-here
GROQ_MODEL=llama-3.3-70b-versatile

# ── Together AI (FREE credits) ────────────────────────────────
# Sign up at: https://api.together.xyz — get $1-$25 free credits
# Supports Mistral 7B (same base model as Hancock fine-tune)
TOGETHER_API_KEY=your-together-key-here
TOGETHER_MODEL=mistralai/Mistral-7B-Instruct-v0.3

# ── OpenRouter (FREE rotating models) ────────────────────────
# Sign up at: https://openrouter.ai — free models available daily
# Single key for 100+ models; append ":free" to model name for free tier
OPENROUTER_API_KEY=sk-or-your-key-here
OPENROUTER_MODEL=meta-llama/llama-3.3-70b-instruct:free

# ── Ollama (Local — zero cost self-hosted) ────────────────────
# Install Ollama: https://ollama.com/download
# Then pull a model: ollama pull llama3.1:8b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
OLLAMA_CODER_MODEL=qwen2.5-coder:7b

# ── NVIDIA NIM (Optional cloud backend) ───────────────────────
# Set HANCOCK_LLM_BACKEND=nvidia and provide your key to use NIM instead.
# Get your free API key at: https://build.nvidia.com
NVIDIA_API_KEY=nvapi-your-key-here

# ── OpenAI (Fallback) ─────────────────────────────────────────
# Get your API key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_ORG_ID=org-your-org-id-here

# ── Model Selection (NIM / OpenAI overrides) ──────────────────
# Only used when HANCOCK_LLM_BACKEND=nvidia or =openai
HANCOCK_MODEL=mistralai/mistral-7b-instruct-v0.3
HANCOCK_CODER_MODEL=qwen/qwen2.5-coder-32b-instruct

# OpenAI fallback models
OPENAI_MODEL=gpt-4o-mini
OPENAI_CODER_MODEL=gpt-4o

# ── Server Config ─────────────────────────────────────────────
HANCOCK_PORT=5000

# ── API Security ──────────────────────────────────────────────
# Bearer token required on all /v1/* endpoints.
# Leave empty to disable auth (dev/local use only).
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
HANCOCK_API_KEY=

# Max requests per IP per minute (default: 60)
HANCOCK_RATE_LIMIT=60

# ── Webhook Notifications ─────────────────────────────────────
# HMAC-SHA256 signature secret for /v1/webhook (optional — set to enforce signature verification)
# Sign requests: X-Hancock-Signature: sha256=<hmac_hex>
HANCOCK_WEBHOOK_SECRET=

# Slack incoming webhook URL (optional — alerts posted to channel on /v1/webhook)
HANCOCK_SLACK_WEBHOOK=

# Microsoft Teams incoming webhook URL (optional)
HANCOCK_TEAMS_WEBHOOK=

# ── Google Cloud Storage (optional — model backup) ────────────────────
# GCS_BUCKET=cyberviser-models
# GCS_PREFIX=v3
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
