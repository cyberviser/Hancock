{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {"name": "python3", "display_name": "Python 3"},
  "language_info": {"name": "python"},
  "kaggle": {"accelerator": "nvidiaTeslaT4", "isInternetEnabled": true, "language": "python", "sourceType": "notebook"}
 },
 "cells": [
  {"cell_type":"markdown","metadata":{},"source":"# üõ°Ô∏è Hancock Fine-Tuning ‚Äî Kaggle GPU\n**CyberViser** | Free: 30h/week T4 GPU on Kaggle\n\n**Before running:**\n1. Settings ‚Üí Accelerator ‚Üí **GPU T4 x2**\n2. Settings ‚Üí Internet ‚Üí **On**\n3. Add secrets: `HF_TOKEN` (optional)\n\n> ‚ö†Ô∏è Add your HuggingFace token as a Kaggle Secret to push the trained model."},
  {"cell_type":"code","metadata":{},"source":"# Install dependencies\n!pip install 'unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git' -q\n!pip install trl transformers accelerate datasets peft bitsandbytes requests tqdm -q\nprint('‚úÖ Done')","execution_count":null,"outputs":[]},
  {"cell_type":"code","metadata":{},"source":"# Clone repo + build dataset\n!git clone https://github.com/cyberviser/Hancock.git\nimport os; os.chdir('Hancock')\n!python hancock_pipeline.py\n\nfrom pathlib import Path\nlines = Path('data/hancock_v2.jsonl').read_text().strip().splitlines()\nprint(f'‚úÖ {len(lines):,} training samples ready')","execution_count":null,"outputs":[]},
  {"cell_type":"code","metadata":{},"source":"import torch, json\nfrom unsloth import FastLanguageModel\nfrom datasets import Dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    'mistralai/Mistral-7B-Instruct-v0.3',\n    max_seq_length=2048, dtype=None, load_in_4bit=True\n)\nmodel = FastLanguageModel.get_peft_model(\n    model, r=32,\n    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n    lora_alpha=32, lora_dropout=0.05, bias='none',\n    use_gradient_checkpointing='unsloth', random_state=42\n)\nprint('‚úÖ Model + LoRA loaded')","execution_count":null,"outputs":[]},
  {"cell_type":"code","metadata":{},"source":"from pathlib import Path\nraw = [json.loads(l) for l in Path('data/hancock_v2.jsonl').read_text().strip().splitlines()]\ntexts = [tokenizer.apply_chat_template(s['messages'], tokenize=False, add_generation_prompt=False) for s in raw]\nds = Dataset.from_dict({'text': texts}).train_test_split(test_size=0.05, seed=42)\nprint(f'Train: {len(ds[\"train\"]):,} | Eval: {len(ds[\"test\"]):,}')","execution_count":null,"outputs":[]},
  {"cell_type":"code","metadata":{},"source":"trainer = SFTTrainer(\n    model=model, tokenizer=tokenizer,\n    train_dataset=ds['train'], eval_dataset=ds['test'],\n    dataset_text_field='text', max_seq_length=2048, packing=True,\n    args=TrainingArguments(\n        per_device_train_batch_size=2, gradient_accumulation_steps=4,\n        warmup_ratio=0.05, num_train_epochs=3, learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(), bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=20, save_strategy='epoch', output_dir='/kaggle/working/checkpoints',\n        report_to='none', optim='adamw_8bit', weight_decay=0.01,\n        lr_scheduler_type='cosine', seed=42,\n    )\n)\nresult = trainer.train()\nprint(f'‚úÖ Training complete ‚Äî loss: {result.training_loss:.4f}')","execution_count":null,"outputs":[]},
  {"cell_type":"code","metadata":{},"source":"# Save + push to HuggingFace Hub\nmodel.save_pretrained('/kaggle/working/hancock_lora')\ntokenizer.save_pretrained('/kaggle/working/hancock_lora')\nmodel.save_pretrained_gguf('/kaggle/working/hancock_gguf', tokenizer, quantization_method='q4_k_m')\nprint('‚úÖ Saved to /kaggle/working/')\n\nimport os\nhf_token = os.getenv('HF_TOKEN', '')\nif hf_token:\n    model.push_to_hub('cyberviser/hancock-mistral-7b-lora', token=hf_token)\n    tokenizer.push_to_hub('cyberviser/hancock-mistral-7b-lora', token=hf_token)\n    print('‚úÖ Pushed to huggingface.co/cyberviser/hancock-mistral-7b-lora')\nelse:\n    print('‚ÑπÔ∏è  Add HF_TOKEN as Kaggle Secret to push to HuggingFace Hub')","execution_count":null,"outputs":[]}
 ]
}
