{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# \ud83d\udee1\ufe0f Hancock Fine-Tuning \u2014 Kaggle GPU\n**CyberViser** | Free: 30h/week T4 GPU on Kaggle\n\n**Before running:**\n1. Settings \u2192 Accelerator \u2192 **GPU T4 x2**\n2. Settings \u2192 Internet \u2192 **On**\n3. Add secrets: `HF_TOKEN` (optional)\n\n> \u26a0\ufe0f Add your HuggingFace token as a Kaggle Secret to push the trained model."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Install dependencies\n!pip install 'unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git' -q\n!pip install trl transformers accelerate datasets peft bitsandbytes requests tqdm -q\nprint('\u2705 Done')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Clone repo + build v3 dataset (CISA KEV + Atomic Red Team + GHSA)\n!git clone https://github.com/cyberviser/Hancock.git\nimport os; os.chdir('Hancock')\n!python hancock_pipeline.py --phase 3\n\nfrom pathlib import Path\npath = Path('data/hancock_v3.jsonl') if Path('data/hancock_v3.jsonl').exists() else Path('data/hancock_v2.jsonl')\nlines = path.read_text().strip().splitlines()\nprint(f'\\u2705 {len(lines):,} training samples ready from {path.name}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch, json\nfrom unsloth import FastLanguageModel\nfrom datasets import Dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    'mistralai/Mistral-7B-Instruct-v0.3',\n    max_seq_length=2048, dtype=None, load_in_4bit=True\n)\nmodel = FastLanguageModel.get_peft_model(\n    model, r=32,\n    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n    lora_alpha=32, lora_dropout=0.05, bias='none',\n    use_gradient_checkpointing='unsloth', random_state=42\n)\nprint('\u2705 Model + LoRA loaded')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from pathlib import Path\npath = Path('data/hancock_v3.jsonl') if Path('data/hancock_v3.jsonl').exists() else Path('data/hancock_v2.jsonl')\nraw = [json.loads(l) for l in path.read_text().strip().splitlines()]\ntexts = [tokenizer.apply_chat_template(s['messages'], tokenize=False, add_generation_prompt=False) for s in raw]\nds = Dataset.from_dict({'text': texts}).train_test_split(test_size=0.05, seed=42)\nprint(f'Train: {len(ds[\"train\"]):,} | Eval: {len(ds[\"test\"]):,}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "trainer = SFTTrainer(\n    model=model, tokenizer=tokenizer,\n    train_dataset=ds['train'], eval_dataset=ds['test'],\n    dataset_text_field='text', max_seq_length=2048, packing=True,\n    args=TrainingArguments(\n        per_device_train_batch_size=2, gradient_accumulation_steps=4,\n        warmup_ratio=0.05, num_train_epochs=3, learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(), bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=20, save_strategy='epoch', output_dir='/kaggle/working/checkpoints',\n        report_to='none', optim='adamw_8bit', weight_decay=0.01,\n        lr_scheduler_type='cosine', seed=42,\n    )\n)\nresult = trainer.train()\nprint(f'\u2705 Training complete \u2014 loss: {result.training_loss:.4f}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Save + push to HuggingFace Hub\nmodel.save_pretrained('/kaggle/working/hancock_lora')\ntokenizer.save_pretrained('/kaggle/working/hancock_lora')\nmodel.save_pretrained_gguf('/kaggle/working/hancock_gguf', tokenizer, quantization_method='q4_k_m')\nprint('\u2705 Saved to /kaggle/working/')\n\nimport os\nhf_token = os.getenv('HF_TOKEN', '')\nif hf_token:\n    model.push_to_hub('cyberviser/hancock-mistral-7b-lora', token=hf_token)\n    tokenizer.push_to_hub('cyberviser/hancock-mistral-7b-lora', token=hf_token)\n    print('\u2705 Pushed to huggingface.co/cyberviser/hancock-mistral-7b-lora')\nelse:\n    print('\u2139\ufe0f  Add HF_TOKEN as Kaggle Secret to push to HuggingFace Hub')",
   "execution_count": null,
   "outputs": []
  }
 ]
}