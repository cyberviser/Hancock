{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hancock Fine-Tune — CyberViser\n",
    "**LoRA fine-tune of Mistral 7B Instruct on the Hancock pentest+SOC dataset**\n",
    "\n",
    "Runtime: `Runtime → Change runtime type → T4 GPU` (free tier is fine)\n",
    "\n",
    "Steps:\n",
    "1. Install dependencies\n",
    "2. Upload `hancock_v2.jsonl`\n",
    "3. Load Mistral 7B with 4-bit quantization\n",
    "4. Apply LoRA adapter\n",
    "5. Train with early stopping\n",
    "6. Save & download adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 1: Check GPU ─────────────────────────────────────────────────────────\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "print(result.stdout if result.returncode == 0 else 'No GPU found — change runtime to T4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 2: Install dependencies ──────────────────────────────────────────────\n",
    "# This takes ~3-4 minutes on first run\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q trl transformers accelerate datasets peft bitsandbytes\n",
    "print('✅ Dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 3: Upload dataset ────────────────────────────────────────────────────\n",
    "# Upload hancock_v2.jsonl from your machine\n",
    "from google.colab import files\n",
    "print('Upload hancock_v2.jsonl from: /home/kalibodo/cyberviser/data/hancock_v2.jsonl')\n",
    "uploaded = files.upload()\n",
    "\n",
    "import json, pathlib\n",
    "dataset_path = pathlib.Path('hancock_v2.jsonl')\n",
    "samples = [json.loads(l) for l in dataset_path.read_text().splitlines() if l.strip()]\n",
    "print(f'✅ Loaded {len(samples):,} samples')\n",
    "print('Sample keys:', list(samples[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: Config ────────────────────────────────────────────────────────────\n",
    "MODEL_NAME      = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "MAX_SEQ_LENGTH  = 4096\n",
    "LORA_R          = 16\n",
    "LORA_ALPHA      = 32\n",
    "MAX_STEPS       = 300   # ~1hr on T4 — increase to 500 for better results\n",
    "BATCH_SIZE      = 2\n",
    "GRAD_ACCUM      = 4     # effective batch = 8\n",
    "LEARNING_RATE   = 2e-4\n",
    "ES_PATIENCE     = 3     # early stopping patience\n",
    "OUTPUT_DIR      = 'hancock-adapter'\n",
    "\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "print(f'LoRA rank: {LORA_R} | Max steps: {MAX_STEPS} | Effective batch: {BATCH_SIZE * GRAD_ACCUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 5: Load model with 4-bit quantization ────────────────────────────────\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f'Loading {MODEL_NAME} with 4-bit quantization...')\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype          = None,      # auto-detect\n",
    "    load_in_4bit   = True,\n",
    ")\n",
    "print('✅ Model loaded')\n",
    "print(f'GPU memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 6: Apply LoRA adapter ────────────────────────────────────────────────\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r                   = LORA_R,\n",
    "    target_modules      = ['q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
    "                            'gate_proj', 'up_proj', 'down_proj'],\n",
    "    lora_alpha          = LORA_ALPHA,\n",
    "    lora_dropout        = 0.05,\n",
    "    bias                = 'none',\n",
    "    use_gradient_checkpointing = 'unsloth',\n",
    "    random_state        = 42,\n",
    ")\n",
    "print(f'✅ LoRA applied — trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 7: Prepare dataset ───────────────────────────────────────────────────\n",
    "from datasets import Dataset\n",
    "\n",
    "def apply_chat_template(sample):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        sample['messages'],\n",
    "        tokenize              = False,\n",
    "        add_generation_prompt = False,\n",
    "    )\n",
    "    return {'text': text}\n",
    "\n",
    "dataset = Dataset.from_list(samples)\n",
    "dataset = dataset.map(apply_chat_template, remove_columns=['messages'])\n",
    "\n",
    "# Filter out samples that are too long\n",
    "def length_filter(sample):\n",
    "    return len(tokenizer.encode(sample['text'])) <= MAX_SEQ_LENGTH\n",
    "\n",
    "before = len(dataset)\n",
    "dataset = dataset.filter(length_filter)\n",
    "print(f'Samples: {before:,} → {len(dataset):,} (after length filter)')\n",
    "\n",
    "split = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split['train']\n",
    "eval_dataset  = split['test']\n",
    "print(f'Train: {len(train_dataset):,} | Eval: {len(eval_dataset):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 8: Train ─────────────────────────────────────────────────────────────\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "import pathlib\n",
    "\n",
    "pathlib.Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = OUTPUT_DIR,\n",
    "    num_train_epochs            = 1,\n",
    "    max_steps                   = MAX_STEPS,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    gradient_accumulation_steps = GRAD_ACCUM,\n",
    "    warmup_steps                = 20,\n",
    "    learning_rate               = LEARNING_RATE,\n",
    "    fp16                        = not torch.cuda.is_bf16_supported(),\n",
    "    bf16                        = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps               = 10,\n",
    "    evaluation_strategy         = 'steps',\n",
    "    eval_steps                  = 50,\n",
    "    save_strategy               = 'steps',\n",
    "    save_steps                  = 50,\n",
    "    load_best_model_at_end      = True,\n",
    "    metric_for_best_model       = 'eval_loss',\n",
    "    greater_is_better           = False,\n",
    "    optim                       = 'adamw_8bit',\n",
    "    weight_decay                = 0.01,\n",
    "    lr_scheduler_type           = 'cosine',\n",
    "    report_to                   = 'none',\n",
    "    run_name                    = 'hancock-v2',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model              = model,\n",
    "    tokenizer          = tokenizer,\n",
    "    train_dataset      = train_dataset,\n",
    "    eval_dataset       = eval_dataset,\n",
    "    dataset_text_field = 'text',\n",
    "    max_seq_length     = MAX_SEQ_LENGTH,\n",
    "    args               = training_args,\n",
    "    callbacks          = [EarlyStoppingCallback(early_stopping_patience=ES_PATIENCE)],\n",
    ")\n",
    "\n",
    "print(f'Starting training — {MAX_STEPS} steps (early stopping patience: {ES_PATIENCE} evals)...')\n",
    "print(f'Effective batch size: {BATCH_SIZE * GRAD_ACCUM}')\n",
    "trainer_stats = trainer.train()\n",
    "print(f'\\n✅ Training complete in {trainer_stats.metrics[\"train_runtime\"]:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 9: Evaluate final loss ───────────────────────────────────────────────\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"Final eval loss : {metrics['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity      : {2 ** metrics['eval_loss']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 10: Test inference ───────────────────────────────────────────────────\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "SYSTEM = \"\"\"You are Hancock, an elite penetration tester and offensive security specialist built by CyberViser.\n",
    "You operate STRICTLY within authorized scope. You always confirm authorization before suggesting active techniques.\"\"\"\n",
    "\n",
    "test_prompt = \"How do I perform Kerberoasting on an authorized Active Directory environment?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\",    \"content\": SYSTEM},\n",
    "    {\"role\": \"user\",      \"content\": test_prompt},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize              = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors        = 'pt',\n",
    ").to('cuda')\n",
    "\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "print(f'\\n[Hancock] {test_prompt}\\n')\n",
    "_ = model.generate(\n",
    "    input_ids  = inputs,\n",
    "    streamer   = streamer,\n",
    "    max_new_tokens = 512,\n",
    "    temperature    = 0.7,\n",
    "    top_p          = 0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 11: Save adapter ─────────────────────────────────────────────────────\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f'✅ Adapter saved to {OUTPUT_DIR}/')\n",
    "\n",
    "import os\n",
    "files_saved = os.listdir(OUTPUT_DIR)\n",
    "print('Files:', files_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 12: Download adapter as zip ─────────────────────────────────────────\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_path = shutil.make_archive('hancock-adapter', 'zip', OUTPUT_DIR)\n",
    "print(f'Zipped: {zip_path}')\n",
    "files.download('hancock-adapter.zip')\n",
    "print('✅ Download started — save to /home/kalibodo/cyberviser/hancock-adapter/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 13 (Optional): Push to Hugging Face Hub ─────────────────────────────\n",
    "# Uncomment to push adapter to HuggingFace (private repo)\n",
    "\n",
    "# HF_TOKEN = 'hf_...'   # your HuggingFace write token\n",
    "# REPO_ID  = 'your-username/hancock-pentest-adapter'\n",
    "#\n",
    "# from huggingface_hub import login\n",
    "# login(token=HF_TOKEN)\n",
    "#\n",
    "# model.push_to_hub(REPO_ID, private=True)\n",
    "# tokenizer.push_to_hub(REPO_ID, private=True)\n",
    "# print(f'✅ Pushed to https://huggingface.co/{REPO_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After downloading the adapter\n",
    "\n",
    "Extract the zip to `/home/kalibodo/cyberviser/hancock-adapter/`\n",
    "\n",
    "Then load it locally with Ollama or transformers:\n",
    "\n",
    "```python\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = 'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    max_seq_length = 4096,\n",
    "    load_in_4bit   = True,\n",
    ")\n",
    "model.load_adapter('hancock-adapter/')\n",
    "FastLanguageModel.for_inference(model)\n",
    "```\n",
    "\n",
    "Or run the agent (uses NVIDIA NIM — no GPU needed locally):\n",
    "```bash\n",
    "cd /home/kalibodo/cyberviser\n",
    "python hancock_agent.py\n",
    "```"
   ]
  }
 ]
}
